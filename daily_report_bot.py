# æ–‡ä»¶åŸºäºç”¨æˆ·åŸå§‹ä¸Šä¼ ï¼š/mnt/data/111.py
# B+ å¢å¼ºç‰ˆ â€” è§„åˆ™åŒ–æ‘˜è¦ï¼ˆæ—  GPTã€å¯åœ¨ GitHub Actions ç›´æ¥è¿è¡Œï¼‰
# åŠŸèƒ½ï¼šæŠ“å– Google News RSSï¼ˆæ—¥è¯­ï¼‰ -> è§„åˆ™åŒ–ç”Ÿæˆå¯è¯»ä¸­æ–‡æ ‡é¢˜/æ‘˜è¦ -> å‘é€é£ä¹¦å¡ç‰‡
# è¯´æ˜ï¼šå°†æ­¤æ–‡ä»¶ä¿å­˜ä¸º 111_B_plus.py å¹¶åœ¨ GitHub Actions ä¸­è¿è¡Œã€‚è¯·åœ¨ä»“åº“ Secrets ä¸­è®¾ç½® FEISHU_WEBHOOKã€‚

import requests
import json
import time
import datetime
import xml.etree.ElementTree as ET
import os
import re

# ================= é…ç½®åŒºåŸŸ =================
FEISHU_WEBHOOK_URL = os.environ.get("FEISHU_WEBHOOK", "")
TIKTOK_SALES_LINK = "https://www.fastmoss.com/zh/e-commerce/saleslist?region=JP"
JAPAN_NEWS_LIMIT = 10
MAX_RETRIES = 3
INITIAL_BACKOFF = 2

# ================= è¯åº“ä¸æ¨¡æ¿ï¼ˆå¯æ‰©å±•ï¼‰ =================
# è¡Œä¸š/å¸¸ç”¨è¯æ˜ å°„ï¼ˆæ—¥æ–‡ token -> ä¸­æ–‡ï¼‰
BASE_KEYWORD_MAP = {
    # è¡Œä¸š/é¢†åŸŸ
    "è‡ªå‹•è»Š": "æ±½è½¦", "è‡ªå‹•è»Šæ¥­ç•Œ": "æ±½è½¦è¡Œä¸š", "å®¶é›»": "å®¶ç”µ", "é‡‘è": "é‡‘è", "çµŒæ¸ˆ": "ç»æµ",
    "å¸‚å ´": "å¸‚åœº", "è²©å£²": "é”€å”®", "è²©å£²å°æ•°": "é”€é‡", "å¢—åŠ ": "å¢åŠ ", "æ¸›å°‘": "å‡å°‘",
    "å›å¾©": "å›å‡", "äºˆæ¸¬": "é¢„æµ‹", "ç™ºè¡¨": "å‘å¸ƒ", "é€Ÿå ±": "å¿«è®¯", "ç™ºç”Ÿ": "å‘ç”Ÿ",
    "åœ°éœ‡": "åœ°éœ‡", "äº‹æ•…": "äº‹æ•…", "èª¿æŸ»": "è°ƒæŸ¥", "é–‹å§‹": "å¼€å§‹", "çµ‚äº†": "ç»“æŸ",
    "æ”¿åºœ": "æ”¿åºœ", "ä¼æ¥­": "ä¼ä¸š", "å ±å‘Š": "æŠ¥å‘Š", "å…¬è¡¨": "å…¬å¸ƒ", "å‰²å¼•": "æŠ˜æ‰£",
    # å“ç±»/ç”Ÿæ´»ç›¸å…³
    "ã‚³ã‚¹ãƒ¡": "ç¾å¦†", "ã‚¹ã‚­ãƒ³ã‚±ã‚¢": "æŠ¤è‚¤", "é£Ÿå“": "é£Ÿå“", "ã‚°ãƒ«ãƒ¡": "ç¾é£Ÿ", "ã‚¢ã‚¦ãƒˆãƒ‰ã‚¢": "æˆ·å¤–",
    "ãƒ™ãƒ“ãƒ¼": "å©´å„¿", "ã‚­ãƒƒã‚º": "å„¿ç«¥",
    # æŒ‡ç¤ºè¯
    "æœ€æ–°": "æœ€æ–°", "æ³¨ç›®": "å…³æ³¨", "äººæ°—": "çƒ­é—¨", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°": "æ¦œå•",
}

# æ‘˜è¦æ¨¡æ¿
TEMPLATES = {
    'publish': "{subject}{time}å‘å¸ƒ{obj}ã€‚",
    'trend': "{subject}{time}{field}{trend}ã€‚",
    'sales': "{subject}{time}{field}é”€é‡{trend}ã€‚",
    'event': "{subject}{time}å‘ç”Ÿ{event}ï¼Œæœ€æ–°æƒ…å†µã€‚",
    'fallback': "{subject}{time}ç›¸å…³æ¶ˆæ¯æ›´æ–°ã€‚"
}

# ç”¨äºæ¸…æ´—ä¸éœ€è¦çš„åª’ä½“/ä½œè€…å…³é”®è¯
NOISE_WORDS = [
    'æœæ—¥', 'å…±åŒé€šä¿¡', 'æ—¥çµŒ', 'NHK', 'ã‚¹ãƒãƒ¼ãƒ„', 'è¨˜è€…', 'æä¾›', 'PR TIMES', 'é…ä¿¡', 'å†™çœŸ']

# æ—¶é—´è¯æ­£åˆ™
TIME_PATTERNS = [
    (re.compile(r'(\d{4})å¹´'), '{year}å¹´'),
    (re.compile(r'(\d{1,2})æœˆ'), '{month}æœˆ'),
    (re.compile(r'\b(10æœˆ|11æœˆ|12æœˆ)\b'), '{month}'),
]

# ================= è¾…åŠ©å‡½æ•° =================

def safe_text(text):
    return text if text else ""


def remove_noise(text):
    """ç§»é™¤è®°ååª’ä½“ã€PR tagã€å¥‡æ€ªçš„æ‹¬å·ä¸å°¾éƒ¨æ¥æº"""
    if not text:
        return ""
    t = re.sub(r' - [^\-\s]+$', '', text)
    t = re.sub(r'\(.*?\)', ' ', t)
    t = re.sub(r'ã€.*?ã€‘', ' ', t)
    # å»æ‰å¸¸è§åª’ä½“/ä½œè€…è¯
    for w in NOISE_WORDS:
        t = t.replace(w, ' ')
    t = re.sub(r'\s+', ' ', t).strip()
    return t


def extract_time(text):
    """å°è¯•ä»æ ‡é¢˜ä¸­æå–æ—¶é—´ä¿¡æ¯ï¼Œè¿”å›æ ¼å¼åŒ–å­—ç¬¦ä¸²ï¼šå¦‚ ' 10æœˆ' æˆ– ' 2025å¹´' ç­‰"""
    if not text:
        return ''
    # ä¼˜å…ˆæŸ¥æ‰¾ å¹´/æœˆ æ¨¡å¼
    m = re.search(r'(\d{4})å¹´', text)
    if m:
        return f" {m.group(1)}å¹´"
    m2 = re.search(r'(\d{1,2})æœˆ', text)
    if m2:
        return f" {m2.group(1)}æœˆ"
    # æ— æ—¶é—´ä¿¡æ¯è¿”å›ç©º
    return ''


def keep_chinese_hanzi_and_digits(text):
    """ä¿ç•™æ±‰å­—/æ•°å­—/è‹±æ–‡(çŸ­è¯)ï¼Œå»æ‰å‡åå’Œå¤šä½™ç¬¦å·ï¼Œä¾¿äºåç»­å…³é”®è¯åŒ¹é…"""
    if not text:
        return ''
    # å…ˆåˆ é™¤ç‰‡å‡åå’Œå¹³å‡å
    t = re.sub(r'[ã-ã‚“ã‚¡-ãƒ³]', ' ', text)
    # åˆ é™¤ç‰¹æ®Šç¬¦å·ä½†ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—
    t = re.sub(r'[^0-9A-Za-z\u4e00-\u9fa5 ]', ' ', t)
    t = re.sub(r'\s+', ' ', t).strip()
    return t


def map_keywords(tokens):
    """æŠŠ tokensï¼ˆæ±‰å­—æˆ–è¯ï¼‰æ˜ å°„ä¸ºä¸­æ–‡å‹å¥½è¯ï¼Œå¦‚æœæ²¡æœ‰æ˜ å°„å°±åŸæ ·è¿”å›"""
    mapped = []
    for t in tokens:
        if not t:
            continue
        if t in BASE_KEYWORD_MAP:
            mapped.append(BASE_KEYWORD_MAP[t])
        else:
            mapped.append(t)
    # å»é‡å¹¶è¿”å›
    return list(dict.fromkeys(mapped))


def classify_and_assemble(mapped_tokens, raw_text):
    """æ ¹æ®å…³é”®è¯ä¸åŸå§‹æ–‡æœ¬ï¼Œé€‰æ‹©æ¨¡æ¿ç”Ÿæˆä¸­æ–‡æ‘˜è¦"""
    subject = ''
    obj = ''
    field = ''
    trend = ''
    event = ''
    time_str = extract_time(raw_text)

    # ç®€å•è§„åˆ™ï¼šä» mapped_tokens ä¸­å¯»æ‰¾ä¸»ä½“/é¢†åŸŸ/è¶‹åŠ¿è¯
    # ä¸»ä½“ä¼˜å…ˆé€‰æ‹©ï¼šæ—¥æœ¬/æ”¿åºœ/ä¼ä¸š/æœºæ„/åœ°å
    for t in mapped_tokens:
        if t in ('æ—¥æœ¬', 'æ—¥æœ¬å›½å†…', 'ä¸œäº¬', 'å¤§é˜ª'):
            subject = t
            break
    if not subject and mapped_tokens:
        subject = mapped_tokens[0]

    # é¢†åŸŸ/å¯¹è±¡é€‰æ‹©
    for t in mapped_tokens:
        if t in ('æ±½è½¦', 'æ±½è½¦è¡Œä¸š', 'å®¶ç”µ', 'é‡‘è', 'ç»æµ', 'å¸‚åœº', 'ç¾å¦†', 'æŠ¤è‚¤', 'é£Ÿå“'):
            field = t
            break

    # è¶‹åŠ¿è¯
    for t in mapped_tokens:
        if t in ('å¢åŠ ', 'å¢é•¿', 'å›å‡', 'ä¸‹æ»‘', 'å‡å°‘', 'å›è½'):
            trend = 'å›å‡' if t in ('å›å¾©','å›å‡') else t
            break

    # äº‹ä»¶è¯ï¼ˆäº‹æ•…/å‘å¸ƒ/æŠ¥å‘Šï¼‰
    for t in mapped_tokens:
        if t in ('å‘å¸ƒ', 'å…¬å¸ƒ', 'æŠ¥å‘Š', 'å¿«è®¯', 'å‘ç”Ÿ', 'äº‹æ•…', 'åœ°éœ‡', 'è°ƒæŸ¥', 'å¼€å§‹'):
            event = t
            break

    # å¯¹è±¡ï¼ˆå•†å“/é”€é‡ç­‰ï¼‰
    if 'é”€é‡' in mapped_tokens or 'é”€å”®' in mapped_tokens:
        obj = 'é”€é‡'

    # é€‰æ‹©æ¨¡æ¿
    if event and event in ('å‘å¸ƒ', 'å…¬å¸ƒ', 'æŠ¥å‘Š'):
        # publish
        obj_str = obj if obj else (field if field else '')
        return TEMPLATES['publish'].format(subject=subject, time=time_str, obj=obj_str)
    if obj == 'é”€é‡' or field in ('æ±½è½¦','å®¶ç”µ','æŠ¤è‚¤','ç¾å¦†','é£Ÿå“'):
        # sales/trend
        t_word = trend if trend else 'å˜åŒ–'
        return TEMPLATES['sales'].format(subject=subject, time=time_str, field=field if field else '', trend=t_word)
    if event in ('äº‹æ•…','åœ°éœ‡','å‘ç”Ÿ'):
        return TEMPLATES['event'].format(subject=subject, time=time_str, event=event)

    # fallbackï¼šå°è¯•ç”¨å‰å‡ ä¸ªå…³é”®è¯ç»„åˆ
    # ç»„åˆå‰ 4 ä¸ªå…³é”®è¯ï¼Œä¿è¯ä¸ç©º
    comb = ''.join(mapped_tokens[:4]) if mapped_tokens else subject
    return TEMPLATES['fallback'].format(subject=comb, time=time_str)


def normalize_japanese_title_to_chinese_better(text):
    """å¢å¼ºç‰ˆæ ‡é¢˜è§„èŒƒåŒ–ä¸»å‡½æ•°ï¼šç»“åˆæ‰€æœ‰æ­¥éª¤ï¼Œè¾“å‡ºå¯è¯»ä¸­æ–‡çŸ­å¥"""
    if not text:
        return 'å†…å®¹ç¼ºå¤±'

    raw = safe_text(text)
    t = remove_noise(raw)
    t2 = keep_chinese_hanzi_and_digits(t)

    # tok: æå–è¿è´¯çš„ä¸­æ–‡/æ±‰å­— tokenï¼ˆæŒ‰è¿ç»­æ±‰å­—åºåˆ—åˆ†å‰²ï¼‰
    tokens = re.findall(r'[\u4e00-\u9fa5]+', t2)

    # æ˜ å°„è¯åº“
    mapped = map_keywords(tokens)

    # å¦‚æœæ˜ å°„ç»“æœä¸ºç©ºï¼Œä½œä¸º fallback è¿”å›ä¸€ä¸ªç®€åŒ–å¹²å‡€ç‰ˆçš„åŸæ–‡ï¼ˆå»æ‰å‡åä¸å¤šä½™ç¬¦å·ï¼‰
    if not mapped:
        fallback = t2[:60] + ('...' if len(t2) > 60 else '')
        return fallback + 'ã€‚'

    # ç”Ÿæˆæœ€ç»ˆæ‘˜è¦å¥
    summary = classify_and_assemble(mapped, raw)

    # æ¸…ç†é‡å¤ã€è¿ç»­ç›¸ä¼¼è¯
    summary = re.sub(r'(ã€‚){2,}', 'ã€‚', summary)
    summary = re.sub(r'\s+', ' ', summary).strip()

    return summary

# ================= Google RSS æŠ“å– =================

def fetch_google_news_rss(query, limit=5, is_jp_query=True):
    hl = 'ja' if is_jp_query else 'en'
    gl = 'JP' if is_jp_query else 'US'
    ceid = 'JP:ja' if is_jp_query else 'US:en'

    encoded_query = requests.utils.quote(query)
    url = f"https://news.google.com/rss/search?q={encoded_query}&hl={hl}&gl={gl}&ceid={ceid}"

    news_items = []

    for attempt in range(MAX_RETRIES):
        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            resp = requests.get(url, headers=headers, timeout=20)
            resp.raise_for_status()
            root = ET.fromstring(resp.content)

            for item in root.findall('./channel/item')[:limit]:
                title_jp = safe_text(item.find('title').text)
                link = safe_text(item.find('link').text)

                title_cn = normalize_japanese_title_to_chinese_better(title_jp)

                news_items.append({
                    'title_jp': title_jp,
                    'title_cn': title_cn,
                    'link': link
                })

            return news_items

        except Exception as e:
            # æ‰“å°å¼‚å¸¸ä»¥ä¾¿ GitHub Actions æ—¥å¿—æŸ¥çœ‹
            print(f"æŠ“å–å¤±è´¥ (query={query}) ç¬¬ {attempt+1} æ¬¡: {e}")
            if attempt < MAX_RETRIES - 1:
                time.sleep(INITIAL_BACKOFF * (2 ** attempt))

    return []

# ================= æ•°æ®æŠ“å–æ¨¡å— =================

def get_tiktok_sales_ranking():
    return fetch_google_news_rss("TikTok å£²ã‚Œç­‹ å•†å“", limit=5)

def get_tiktok_hashtag_trends():
    items = fetch_google_news_rss("TikTok ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚° ãƒˆãƒ¬ãƒ³ãƒ‰", limit=5, is_jp_query=False)
    for it in items:
        it['title_cn'] = None
    return items

def get_rakuten_ranking_info():
    queries = [
        "æ¥½å¤©å¸‚å ´ ãƒ©ãƒ³ã‚­ãƒ³ã‚° ç·åˆ",
        "æ¥½å¤©å¸‚å ´ ãƒ©ãƒ³ã‚­ãƒ³ã‚° ç¾å®¹",
        "æ¥½å¤©å¸‚å ´ ãƒ©ãƒ³ã‚­ãƒ³ã‚° ã‚°ãƒ«ãƒ¡",
        "æ¥½å¤©å¸‚å ´ ãƒ©ãƒ³ã‚­ãƒ³ã‚° ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³",
        "æ¥½å¤©å¸‚å ´ ãƒ©ãƒ³ã‚­ãƒ³ã‚° å®¶é›»"
    ]
    results = []
    for q in queries:
        results.extend(fetch_google_news_rss(q, limit=1))
    return results


def get_yahoo_ranking_info():
    queries = [
        "Yahoo!ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚° ãƒ©ãƒ³ã‚­ãƒ³ã‚°",
        "Yahoo!ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚° å£²ã‚Œç­‹",
        "Yahoo!ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚° äººæ°—",
        "Yahoo!ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚° æ³¨ç›®",
        "Yahoo!ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚° å®¶é›»"
    ]
    results = []
    for q in queries:
        results.extend(fetch_google_news_rss(q, limit=1))
    return results


def get_japan_real_time_news():
    return fetch_google_news_rss("æ—¥æœ¬ å›½å†… æœ€æ–° ãƒ‹ãƒ¥ãƒ¼ã‚¹", limit=JAPAN_NEWS_LIMIT)

# ================= é£ä¹¦å‘é€ =================

def send_feishu_card(webhook_url, data):
    today = datetime.datetime.now().strftime('%Y-%m-%d')

    def make_list_text(items, is_translated=True):
        if not items:
            return 'æš‚æ— æ•°æ®'
        txt = ''
        for i, item in enumerate(items):
            if is_translated:
                txt += f"{i+1}. **{item['title_cn']}** [æŸ¥çœ‹åŸæ–‡]({item['link']})\n"
            else:
                txt += f"{i+1}. [{item['title_jp']}]({item['link']})\n"
        return txt

    card = {
        'msg_type': 'interactive',
        'card': {
            'header': {
                'title': {'tag': 'plain_text', 'content': f'ğŸ‡¯ğŸ‡µ æ—¥æœ¬ç”µå•†æ—¥æŠ¥ {today}'},
                'template': 'blue'
            },
            'elements': [
                {'tag':'div','text':{'tag':'lark_md','content':'ğŸ”¥ **1. æ—¥æœ¬ TikTok æ˜¨æ—¥é”€é‡æ¦œå•**'}},
                {'tag':'div','text':{'tag':'lark_md','content':make_list_text(data['tiktok_sales'])}},
                {'tag':'hr'},
                {'tag':'div','text':{'tag':'lark_md','content':'ğŸµ **2. TikTok çƒ­é—¨æ ‡ç­¾è¯**'}},
                {'tag':'div','text':{'tag':'lark_md','content':make_list_text(data['tiktok_hashtag'], is_translated=False)}},
                {'tag':'hr'},
                {'tag':'div','text':{'tag':'lark_md','content':'ğŸ”´ **3. ä¹å¤©ç²¾é€‰æ¦œå•**'}},
                {'tag':'div','text':{'tag':'lark_md','content':make_list_text(data['rakuten_ranking'])}},
                {'tag':'hr'},
                {'tag':'div','text':{'tag':'lark_md','content':'ğŸŸ¢ **4. é›…è™è´­ç‰©ç²¾é€‰æ¦œå•**'}},
                {'tag':'div','text':{'tag':'lark_md','content':make_list_text(data['yahoo_ranking'])}},
                {'tag':'hr'},
                {'tag':'div','text':{'tag':'lark_md','content':'ğŸ“° **5. æ—¥æœ¬å®æ—¶æ–°é—»**'}},
                {'tag':'div','text':{'tag':'lark_md','content':make_list_text(data['japan_news'])}},
            ]
        }
    }

    try:
        resp = requests.post(webhook_url, headers={'Content-Type':'application/json'}, data=json.dumps(card))
        print('é£ä¹¦å‘é€å“åº”ï¼š', resp.status_code, resp.text)
    except Exception as e:
        print('å‘é€é£ä¹¦å¤±è´¥ï¼š', e)

# ================= ä¸»ç¨‹åº =================

def main():
    if not FEISHU_WEBHOOK_URL:
        print('é”™è¯¯ï¼šæœªè®¾ç½®é£ä¹¦ Webhookï¼ˆè¯·åœ¨ GitHub ä»“åº“ Secrets ä¸­è®¾ç½® FEISHU_WEBHOOKï¼‰')
        return

    data = {
        'tiktok_sales': get_tiktok_sales_ranking(),
        'tiktok_hashtag': get_tiktok_hashtag_trends(),
        'rakuten_ranking': get_rakuten_ranking_info(),
        'yahoo_ranking': get_yahoo_ranking_info(),
        'japan_news': get_japan_real_time_news()
    }

    send_feishu_card(FEISHU_WEBHOOK_URL, data)


if __name__ == '__main__':
    main()
