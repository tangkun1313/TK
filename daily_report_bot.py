import requests
import json
import time
import datetime
import xml.etree.ElementTree as ET
import os
import re

# ================= ÈÖçÁΩÆÂå∫Âüü =================
FEISHU_WEBHOOK_URL = os.environ.get("FEISHU_WEBHOOK", "")
TIKTOK_SALES_LINK = "https://www.fastmoss.com/zh/e-commerce/saleslist?region=JP"
JAPAN_NEWS_LIMIT = 10 
MAX_RETRIES = 3
INITIAL_BACKOFF = 2

# ================= Âº∫ÂåñÁøªËØëÂáΩÊï∞ÔºàÊúÄÁªàÁâàÔºåÂÖ®‰∏≠ÊñáËæìÂá∫Ôºâ =================

def aggressive_translate_and_clean(text):
    """
    ÊúÄÁªàÁâàÔºöÂº∫Âà∂ 100% ËæìÂá∫‰∏≠Êñá
    Â§ÑÁêÜÊ≠•È™§Ôºö
    1. ÁøªËØëËØçÂÖ∏ÔºàÊó•‚Üí‰∏≠Ôºâ
    2. Ê∏ÖÁêÜÂÅáÂêç
    3. Êó•Êú¨Ê±âÂ≠óËØç ‚Üí ÂØπÂ∫î‰∏≠Êñá
    4. ËøáÊª§Èùû‰∏≠ÊñáÂ≠óÁ¨¶
    """

    if not text:
        return "ÂÜÖÂÆπÁº∫Â§±"

    # Âü∫Á°ÄÈ¢ÑÂ§ÑÁêÜ
    clean_text = re.sub(r' - [^-\s]+$', '', text).strip()
    clean_text = re.sub(r'\(.*?\)', '', clean_text)

    # ========== Êó•Êú¨Ê±âÂ≠óËØç ‚Üí ‰∏≠ÊñáÔºàÊñ∞Â¢ûÔºâ ==========
    jp_to_cn_word_map = {
        "Êù±‰∫¨ÈÉΩ": "‰∏ú‰∫¨",
        "Â§ßÈò™Â∫ú": "Â§ßÈò™",
        "ÂåóÊµ∑ÈÅì": "ÂåóÊµ∑ÈÅì",
        "Á•ûÂ•àÂ∑ùÁúå": "Á•ûÂ•àÂ∑ù",
        "ÂõΩÂÜÖ": "Êó•Êú¨ÂõΩÂÜÖ",
        "Êó•Áµå": "Êó•Êú¨ÁªèÊµéÊñ∞Èóª",
        "Á∑èÂãôÁúÅ": "Êó•Êú¨ÊÄªÂä°ÁúÅ",
        "ÂéöÁîüÂä¥ÂÉçÁúÅ": "Êó•Êú¨ÂéöÁîüÂä≥Âä®ÁúÅ",
        "Â≤∏Áî∞": "Â≤∏Áî∞ÊñáÈõÑ",
        "ÊîøÂ∫ú": "Êó•Êú¨ÊîøÂ∫ú",
        "ËÉΩÁôªÂú∞Èúá": "ËÉΩÁôªÂçäÂ≤õÂú∞Èúá",
        "ÁµåÊ∏à": "ÁªèÊµé",
        "ÂÜÜÂÆâ": "Êó•ÂÖÉË¥¨ÂÄº",
        "ÂÜÜÈ´ò": "Êó•ÂÖÉÂçáÂÄº",
        "Êñ∞Ë¶è": "Êñ∞Â¢û",
        "ÊÑüÊüìËÄÖ": "ÊÑüÊüì‰∫∫Êï∞",
        "ÈÄüÂ†±": "Âø´ËÆØ",
        "Áô∫Áîü": "ÂèëÁîü",
        "‰ºöË¶ã": "ËÆ∞ËÄÖ‰ºö",
        "Ë≠¶ÂØüÂ∫Å": "Êó•Êú¨Ë≠¶ÂØüÂéÖ",
        "Ê∞óË±°Â∫Å": "Êó•Êú¨Ê∞îË±°ÂéÖ",
    }

    for jp, cn in sorted(jp_to_cn_word_map.items(), key=lambda x: len(x[0]), reverse=True):
        clean_text = clean_text.replace(jp, cn)

    translated_text = clean_text

    # ========== Êó• ‚Üí ‰∏≠ ËØçÂÖ∏ÊõøÊç¢Ôºà‰øùÁïô‰Ω†ÂéüÊù•ÁöÑÁøªËØëÈÄªËæëÔºâ ==========
    translation_map = {
        "Ëá™Â∑±Ê∫ÄË∂≥ÂûãÊ∂àË≤ª": "ÊÇ¶Â∑±Ê∂àË¥π",
        "ÊÄ•ÊµÆ‰∏ä": "ËøÖÈÄüÂ¥õËµ∑",
        "Â£≤„ÇåÁ≠ã„É©„É≥„Ç≠„É≥„Ç∞": "ÁÉ≠ÈîÄÊéíË°åÊ¶ú",
        "„Éá„Ç§„É™„Éº„É©„É≥„Ç≠„É≥„Ç∞": "ÊØèÊó•Ê¶úÂçï",
        "„É™„Ç¢„É´„Çø„Ç§„É†": "ÂÆûÊó∂",
        "Êñ∞ÂïÜÂìÅ": "Êñ∞ÂìÅ",
        "ÊàêÂäü‰∫ã‰æã": "ÊàêÂäüÊ°à‰æã",
        "ÁßòË®£": "ËØÄÁ™ç",
        "„Ç≥„Çπ„É°": "ÁæéÂ¶Ü",
        "„Éë„Éº„ÇΩ„Éä„É´„Ç±„Ç¢": "‰∏™‰∫∫Êä§ÁêÜ",
        "„Çπ„Ç≠„É≥„Ç±„Ç¢": "ÁöÆËÇ§Êä§ÁêÜ",
        "„Ç¢„Éë„É¨„É´": "ÊúçÈ•∞",
        "„É©„Ç§„Éï„Çπ„Çø„Ç§„É´": "ÁîüÊ¥ªÊñπÂºè",
        "„Éá„Ç∏„Çø„É´": "Êï∞Á†Å",
        "ÊúÄÊñ∞ÂãïÂêë": "ÊúÄÊñ∞Âä®ÊÄÅ",
        "„É©„É≥„Ç≠„É≥„Ç∞ÂÖ•„Çä": "ËøõÂÖ•Ê¶úÂçï",
        "Ë≤©Â£≤": "ÈîÄÂîÆ",
        "Â∞èÂ£≤": "Èõ∂ÂîÆ",
        "Êà¶Áï•": "ÊàòÁï•",
        "Ê≥®ÁõÆ": "ÂÖ≥Ê≥®",
        "„É¨„Éù„Éº„Éà": "Êä•Âëä",
        "„Éñ„É©„É≥„Éâ": "ÂìÅÁâå",
        "„Ç∑„Éß„ÉÉ„Éó": "Â∫óÈì∫",
        "„Çª„Éº„É´": "‰øÉÈîÄ",
        "„Ç≠„É£„É≥„Éö„Éº„É≥": "Ê¥ªÂä®",
        "ÁæéÈ£ü": "ÁæéÈ£ü",
        "‰∫∫Ê∞ó": "ÁÉ≠Èó®",
        "Â∏ÇÂ†¥": "Â∏ÇÂú∫",
        "ÂÆ∂Áîµ": "ÂÆ∂Áîµ",
        "È£üÂìÅ": "È£üÂìÅ",
        "ÈõëË≤®": "ÊùÇË¥ß",
    }

    for jp, cn in sorted(translation_map.items(), key=lambda x: len(x[0]), reverse=True):
        translated_text = translated_text.replace(jp, cn)

    # ========== Âà†Èô§ÊâÄÊúâÂÅáÂêç ==========
    translated_text = re.sub(r'[„ÅÅ-„Çì„Ç°-„É≥]', '', translated_text)

    # ========== ÊúÄÁªàËøáÊª§ÔºöÂè™‰øùÁïô‰∏≠Êñá/Êï∞Â≠ó/Ê†áÁÇπ ==========
    translated_text = re.sub(r'[^0-9\u4e00-\u9fa5Ôºå„ÄÇÔºÅÔºü‚Ä¶]', ' ', translated_text)
    translated_text = re.sub(r'\s+', ' ', translated_text).strip()

    if len(translated_text) > 45:
        translated_text = translated_text[:45] + "..."

    return translated_text


# ================= Google RSS ÊäìÂèñ =================

def fetch_google_news_rss(query, limit=5, is_jp_query=True):
    hl = 'ja' if is_jp_query else 'en'
    gl = 'JP' if is_jp_query else 'US'
    ceid = 'JP:ja' if is_jp_query else 'US:en'
    
    encoded_query = requests.utils.quote(query)
    url = f"https://news.google.com/rss/search?q={encoded_query}&hl={hl}&gl={gl}&ceid={ceid}"
    news_items = []

    for attempt in range(MAX_RETRIES):
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0'
            }
            response = requests.get(url, headers=headers, timeout=20)
            response.raise_for_status()
            root = ET.fromstring(response.content)

            for item in root.findall('./channel/item')[:limit]:
                title_jp = item.find('title').text
                link = item.find('link').text
                title_cn = aggressive_translate_and_clean(title_jp)

                news_items.append({
                    "title_jp": title_jp,
                    "title_cn": title_cn,
                    "link": link
                })
            return news_items
        
        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                time.sleep(INITIAL_BACKOFF * (2 ** attempt))

    return []


# ================= Êï∞ÊçÆÊäìÂèñÊ®°Âùó =================

def get_tiktok_sales_ranking():
    return fetch_google_news_rss("TikTok Â£≤„ÇåÁ≠ã ÂïÜÂìÅ", limit=5)

def get_tiktok_hashtag_trends():
    items = fetch_google_news_rss("TikTok „Éè„ÉÉ„Ç∑„É•„Çø„Ç∞ „Éà„É¨„É≥„Éâ", limit=5, is_jp_query=False)
    for item in items:
        item["title_cn"] = None  
    return items

def get_rakuten_ranking_info():
    queries = [
        "Ê•ΩÂ§©Â∏ÇÂ†¥ „É©„É≥„Ç≠„É≥„Ç∞ Á∑èÂêà 1‰Ωç",
        "Ê•ΩÂ§©Â∏ÇÂ†¥ „É©„É≥„Ç≠„É≥„Ç∞ ÁæéÂÆπ",
        "Ê•ΩÂ§©Â∏ÇÂ†¥ „É©„É≥„Ç≠„É≥„Ç∞ „Ç∞„É´„É°",
        "Ê•ΩÂ§©Â∏ÇÂ†¥ „É©„É≥„Ç≠„É≥„Ç∞ „Éï„Ç°„ÉÉ„Ç∑„Éß„É≥",
        "Ê•ΩÂ§©Â∏ÇÂ†¥ „É©„É≥„Ç≠„É≥„Ç∞ ÂÆ∂Èõª"
    ]
    results = []
    for q in queries:
        results.extend(fetch_google_news_rss(q, limit=1))
    return results[:5]

def get_yahoo_ranking_info():
    queries = [
        "Yahoo!„Ç∑„Éß„ÉÉ„Éî„É≥„Ç∞ „É©„É≥„Ç≠„É≥„Ç∞ 1‰Ωç",
        "Yahoo!„Ç∑„Éß„ÉÉ„Éî„É≥„Ç∞ „É©„É≥„Ç≠„É≥„Ç∞ Â∑•ÂÖ∑",
        "Yahoo!„Ç∑„Éß„ÉÉ„Éî„É≥„Ç∞ „É©„É≥„Ç≠„É≥„Ç∞ „Ç¢„Ç¶„Éà„Éâ„Ç¢",
        "Yahoo!„Ç∑„Éß„ÉÉ„Éî„É≥„Ç∞ „É©„É≥„Ç≠„É≥„Ç∞ „Éô„Éì„Éº",
        "Yahoo!„Ç∑„Éß„ÉÉ„Éî„É≥„Ç∞ „É©„É≥„Ç≠„É≥„Ç∞ ÈõëË≤®"
    ]
    results = []
    for q in queries:
        results.extend(fetch_google_news_rss(q, limit=1))
    return results[:5]

def get_japan_real_time_news():
    return fetch_google_news_rss("Êó•Êú¨ ÂõΩÂÜÖ ÊúÄÊñ∞ „Éã„É•„Éº„Çπ", limit=10)


# ================= È£û‰π¶ÂèëÈÄÅ =================

def send_feishu_card(webhook_url, data):
    today = datetime.datetime.now().strftime("%Y-%m-%d")

    def make_list_text(items, is_translated=True):
        if not items:
            return "ÊöÇÊó†Êï∞ÊçÆ"

        txt = ""
        for i, item in enumerate(items):
            link = item["link"]
            if is_translated:
                txt += f"{i+1}. **{item['title_cn']}** [Êü•ÁúãÂéüÊñá]({link})\n"
            else:
                txt += f"{i+1}. [{item['title_jp']}]({link})\n"
        return txt

    card = {
        "msg_type": "interactive",
        "card": {
            "header": {
                "title": {"tag": "plain_text", "content": f"üáØüáµ Êó•Êú¨ÁîµÂïÜÊó•Êä• {today}"},
                "template": "blue"
            },
            "elements": [
                {"tag":"div","text":{"tag":"lark_md","content":"üî• **1. Êó•Êú¨ TikTok Êò®Êó•ÈîÄÈáèÊ¶úÂçï**"}},
                {"tag":"div","text":{"tag":"lark_md","content":make_list_text(data["tiktok_sales"])}},
                {"tag":"hr"},
                {"tag":"div","text":{"tag":"lark_md","content":"üéµ **2. TikTok ÁÉ≠Èó®Ê†áÁ≠æËØç**"}},
                {"tag":"div","text":{"tag":"lark_md","content":make_list_text(data["tiktok_hashtag"], is_translated=False)}},
                {"tag":"hr"},
                {"tag":"div","text":{"tag":"lark_md","content":"üî¥ **3. ‰πêÂ§©Á≤æÈÄâÊ¶úÂçï**"}},
                {"tag":"div","text":{"tag":"lark_md","content":make_list_text(data["rakuten_ranking"])}},
                {"tag":"hr"},
                {"tag":"div","text":{"tag":"lark_md","content":"üü¢ **4. ÈõÖËôéË¥≠Áâ©Á≤æÈÄâÊ¶úÂçï**"}},
                {"tag":"div","text":{"tag":"lark_md","content":make_list_text(data["yahoo_ranking"])}},
                {"tag":"hr"},
                {"tag":"div","text":{"tag":"lark_md","content":"üì∞ **5. Êó•Êú¨ÂÆûÊó∂Êñ∞Èóª**"}},
                {"tag":"div","text":{"tag":"lark_md","content":make_list_text(data["japan_news"])}},
            ]
        }
    }

    requests.post(webhook_url, headers={"Content-Type":"application/json"}, data=json.dumps(card))


# ================= ‰∏ªÁ®ãÂ∫è =================

def main():
    if not FEISHU_WEBHOOK_URL:
        print("ÈîôËØØÔºöÊú™ËÆæÁΩÆÈ£û‰π¶ Webhook")
        return

    data = {
        "tiktok_sales": get_tiktok_sales_ranking(),
        "tiktok_hashtag": get_tiktok_hashtag_trends(),
        "rakuten_ranking": get_rakuten_ranking_info(),
        "yahoo_ranking": get_yahoo_ranking_info(),
        "japan_news": get_japan_real_time_news()
    }

    send_feishu_card(FEISHU_WEBHOOK_URL, data)


if __name__ == "__main__":
    main()
